

#  Automatiser des projets Scrapy avec Apache Airflow

---

## **Objectif gÃ©nÃ©ral**
Automatiser **3 projets Scrapy** grÃ¢ce Ã  **Apache Airflow** installÃ© localement avec Docker, pour exÃ©cuter des extractions de donnÃ©es web de faÃ§on rÃ©guliÃ¨re, maintenable et Ã©volutive.

---

## Module 0 â€“ Introduction Ã  Apache Airflow

### Qu'est-ce qu'Airflow ?
Apache Airflow est un **orchestrateur de workflows** open-source. Il permet dâ€™automatiser des processus mÃ©tiers sous forme de pipelines de tÃ¢ches appelÃ©s **DAGs** (*Directed Acyclic Graphs*).

Chaque DAG :
- est Ã©crit en Python.
- contient des **tÃ¢ches** unitaires (appelÃ©es operators).
- permet de dÃ©finir des dÃ©pendances logiques entre ces tÃ¢ches.

### Pourquoi Airflow pour du Scraping ?
- Lancer des spiders automatiquement (par heure, jour, semaineâ€¦).
- GÃ©rer les logs, les erreurs, les relances.
- IntÃ©grer dâ€™autres Ã©tapes (nettoyage, base de donnÃ©esâ€¦).
- Orchestration simple **mÃªme entre plusieurs projets Scrapy**.

---

## Module 1 â€“ PrÃ©parer tes projets Scrapy

### Structure recommandÃ©e :
```
web-scraping/
â”‚
â”œâ”€â”€ scrapy_project_1/
â”‚   â””â”€â”€ quotes_scraper/
â”‚       â””â”€â”€ spiders/
â”‚           â””â”€â”€ quotes.py
â”œâ”€â”€ scrapy_project_2/
â”‚   â””â”€â”€ news_scraper/
â”œâ”€â”€ scrapy_project_3/
â”‚   â””â”€â”€ jobs_scraper/
â””â”€â”€ airflow/
    â””â”€â”€ dags/
        â””â”€â”€ scraper_dag.py
```

### Ã€ vÃ©rifier dans chaque projet :
- Tu dois pouvoir **exÃ©cuter le spider via une fonction Python** (voir module suivant).
- Chaque projet doit avoir :
  - `scrapy.cfg`
  - Un dossier `spiders/`
  - Un fichier `settings.py`
  - Un fichier `run_spider.py`(Pour lancer le spider automatiquement)

---

## Module 2 â€“ ExÃ©cuter Scrapy depuis Python

### Pourquoi ?
Airflow ne peut pas exÃ©cuter `scrapy crawl spider_name` en ligne de commande facilement si les projets sont multiples. On encapsule donc lâ€™appel dans une fonction Python.

### Exemple : `quotes_scraper/run_spider.py`
```python
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from quotes_scraper.spiders.quotes import QuotesSpider

def run():
    process = CrawlerProcess(get_project_settings())
    process.crawl(QuotesSpider)
    process.start()
```

Tu fais de mÃªme pour les deux autres projets : `news_scraper` et `jobs_scraper`.

---

## â˜ï¸ **Module 3 â€” Installer Apache Airflow en local avec Docker**

---

### ðŸŽ¯ Objectif du module

Mettre en place **Apache Airflow sur ton ordinateur** grÃ¢ce Ã  Docker et Docker Compose, pour quâ€™il soit prÃªt Ã  orchestrer tes spiders Scrapy, **comme sâ€™il tournait dans le cloud**.

---

### ðŸ§  Pourquoi Docker ?

Apache Airflow est composÃ© de plusieurs services qui doivent communiquer entre eux :

Une base de donnÃ©es pour stocker les tÃ¢ches et lâ€™historique
Un serveur web (interface Airflow)
Un planificateur (scheduler) qui dÃ©clenche les DAGs
Ã‰ventuellement un worker (si on scale)
Etâ€¦ des logs, des plugins, etc.
ðŸ‘‰ ProblÃ¨me :
Installer tout Ã§a â€œÃ  la mainâ€ peut vite devenir complexe.

âœ… Solution :
Docker permet dâ€™avoir un environnement complet et prÃªt Ã  lâ€™emploi, avec tous les services dÃ©jÃ  configurÃ©s, que tu peux dÃ©marrer/arrÃªter en une ligne de commande.

---

### ðŸ§ª Ã‰tape 0 â€“ VÃ©rifier les prÃ©requis

Tu dois avoir ces outils installÃ©s :

| Outil | VÃ©rification |
|-------|--------------|
| Docker Desktop | `docker --version` |
| Docker Compose | `docker-compose --version` |
| Git *(optionnel)* | `git --version` |

> Si tu ne les as pas, installe **[Docker Desktop pour Mac](https://www.docker.com/products/docker-desktop/)** puis redÃ©marre ton ordi.

---

### ðŸ§± Ã‰tape 1 â€“ CrÃ©er le dossier de ton projet Airflow

Ouvre ton terminal :

```bash
mkdir airflow-local
cd airflow-local
mkdir dags logs plugins
```

- `dags/` â†’ tes DAGs Airflow
- `logs/` â†’ les fichiers de logs de chaque tÃ¢che
- `plugins/` â†’ pour ajouter des extensions plus tard

---

### ðŸ§¾ Ã‰tape 2 â€“ CrÃ©er le fichier `docker-compose.yml`

Toujours dans le dossier `airflow-local`, crÃ©e un fichier :
```bash
touch docker-compose.yml
```

Ce fichier dÃ©crit les services Docker Ã  dÃ©marrer pour exÃ©cuter Airflow.

Voici les services que tu as dÃ©finis :

| Service        | RÃ´le                                                    |
|----------------|---------------------------------------------------------|
| `postgres`     | La base de donnÃ©es dâ€™Airflow                            |
| `webserver`    | Lâ€™interface web (http://localhost:8080)                |
| `scheduler`    | Le moteur qui exÃ©cute les DAGs automatiquement         |
| `airflow-init` | Initialise Airflow (migrations + crÃ©ation de l'utilisateur) |


Colle dedans ce contenu :

```yaml
version: '3'
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  webserver:
    image: apache/airflow:2.9.1-python3.10
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: 'fernet_key_dev_123'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    depends_on:
      - postgres
    command: webserver

  scheduler:
    image: apache/airflow:2.9.1-python3.10
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - webserver
    command: scheduler

  airflow-init:
    image: apache/airflow:2.9.1-python3.10
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: 'fernet_key_dev_123'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    entrypoint: >
      bash -c "
      airflow db migrate &&
      airflow users create --username airflow --password airflow --firstname Khadija --lastname Aassi --role Admin --email khadija@example.com
      "

volumes:
  postgres-db-volume:
```

---

### âš™ï¸ Ã‰tape 3 â€“ Initialiser Airflow

Lance cette commande une fois pour initialiser la base de donnÃ©es interne dâ€™Airflow :

```bash
docker-compose run --rm airflow-init
```
âœ… Elle fait deux choses :

airflow db migrate â†’ crÃ©e les tables dans la base PostgreSQL (base de donnÃ©es interne dâ€™Airflow)
airflow users create â†’ crÃ©e ton compte admin pour te connecter Ã  lâ€™interface

---

### ðŸš€ Ã‰tape 4 â€“ Lancer Airflow

```bash
docker-compose up
```
Cette commande :

dÃ©marre le serveur web (webserver)
dÃ©marre le planificateur (scheduler)
connecte tout Ã  la base de donnÃ©es PostgreSQL
ðŸ“ Tu accÃ¨des Ã  lâ€™interface sur : http://localhost:8080

Tu peux voir :

Tous les DAGs actifs
Leur historique
Les logs
Les exÃ©cutions manuelles ou planifiÃ©es

### ðŸ§  Comment Airflow fonctionne-t-il ?

â›“ï¸ Le rÃ´le des composants :

| Composant       | Fonction                                                                 |
|-----------------|--------------------------------------------------------------------------|
| **DAG**         | Script Python qui dÃ©finit les tÃ¢ches Ã  exÃ©cuter et leur ordre             |
| **Operator**    | Une tÃ¢che Ã  exÃ©cuter (ex: fonction Python, commande bash, etc.)           |
| **Scheduler**   | Lit les DAGs, planifie et dÃ©clenche les exÃ©cutions                       |
| **Webserver**   | Affiche les DAGs, permet de les activer, de consulter les logs           |
| **Base de donnÃ©es** | Enregistre les DAGs, les exÃ©cutions, les logs, les utilisateurs     |


---

### ðŸ” Connexion Ã  l'interface

Identifiants par dÃ©faut :
- **Login** : `airflow`
- **Mot de passe** : `airflow`

---

### ðŸ§ª Ã‰tape 5 â€“ Tester avec un DAG de base

CrÃ©e un fichier `dags/hello_airflow.py` :

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

with DAG("hello_airflow",
         start_date=datetime(2023, 1, 1),
         schedule_interval=None,
         catchup=False) as dag:

    def hello():
        print("Coucou Khadija, Airflow tourne bien !")

    task = PythonOperator(
        task_id="say_hello",
        python_callable=hello
    )
```

Dans lâ€™interface Airflow :
1. Active le DAG "hello_airflow"
2. Clique sur â–¶ï¸ pour lancer une exÃ©cution
3. Consulte les logs pour vÃ©rifier l'exÃ©cution

---

### ðŸ“Œ RÃ©sumÃ© de cette mise en place

| Ã‰tape | RÃ©sultat |
|-------|----------|
| Docker installÃ© | âœ… environnement conteneurisÃ© |
| `docker-compose.yml` | âœ… Airflow prÃªt Ã  l'emploi |
| `airflow-init` | âœ… Base de donnÃ©es initialisÃ©e |
| `localhost:8080` | âœ… Interface Airflow accessible |
| `dags/` | âœ… PrÃªt Ã  accueillir tes DAGs Scrapy |


### ðŸ“Œ RÃ©sumÃ© visuel du systÃ¨me

                        +--------------------+
                       |    Interface Web   |  â† http://localhost:8080
                       +--------------------+
                                 â†‘
                                 |
                                 â†“
    +----------+   â†’     +-----------+   â†   +-------------+
    |  DAG.py  |         | Scheduler |       | Base de     |
    | (Python) |   â†’     |           |   â†’   | DonnÃ©es     |
    +----------+         +-----------+       +-------------+
                                 â†‘
                                 â†“
                         +------------------+
                         | PythonOperator   |
                         | BashOperator     |
                         | (et autres)      |
                         +------------------+

ðŸ“˜ Ce schÃ©ma illustre comment Airflow fonctionne en interne : chaque composant joue un rÃ´le clÃ© dans l'exÃ©cution des DAGs.

---

âœ… Ã€ ce stade, tu as Airflow local prÃªt Ã  recevoir tes DAGs.  
âž¡ï¸ Passons maintenant Ã  lâ€™automatisation de tes spiders Scrapy avec Airflow.

---

## ðŸª Module 4 â€“ Ã‰crire un DAG pour 3 projets Scrapy

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys

BASE_PATH = "/opt/airflow"

default_args = {
    "owner": "khadija",
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

def add_path(path):
    if path not in sys.path:
        sys.path.append(path)

def run_project(path, spider):
    add_path(path)
    mod = __import__(f"{spider}.run_spider", fromlist=["run"])
    mod.run()

with DAG(
    dag_id="multi_scrapy_dag",
    default_args=default_args,
    start_date=datetime(2024, 4, 1),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    t1 = PythonOperator(
        task_id="scrape_quotes",
        python_callable=run_project,
        op_args=[f"{BASE_PATH}/scrapy_project_1", "quotes_scraper"]
    )

    t2 = PythonOperator(
        task_id="scrape_news",
        python_callable=run_project,
        op_args=[f"{BASE_PATH}/scrapy_project_2", "news_scraper"]
    )

    t3 = PythonOperator(
        task_id="scrape_jobs",
        python_callable=run_project,
        op_args=[f"{BASE_PATH}/scrapy_project_3", "jobs_scraper"]
    )

    t1 >> t2 >> t3
```

---

## Module 5 â€“ Logs & Monitoring

- Tous les logs sont accessibles dans `/opt/airflow/logs/`.
- Tu peux :
  - Stocker ces logs dans **Azure Blob Storage**
  - IntÃ©grer **Grafana ou Prometheus**
  - Ajouter des **notifications Slack ou email**

---

## Module 6 â€“ Bonnes pratiques

- Utilise des **fichiers `.env`** pour les chemins et identifiants.
- Centralise les **exports Scrapy** (JSON, CSV) dans un dossier `/data/`.
- VÃ©rifie tes spiders indÃ©pendamment dâ€™Airflow avant lâ€™intÃ©gration.
- Utilise un fichier `__init__.py` dans tous tes dossiers pour faciliter lâ€™import Python.

---

## Module 7 â€“ Aller plus loin

- **Capteurs (Sensors)** : lancer un scraping seulement si un fichier est disponible ou une API rÃ©pond.
- **ParamÃ¨tres dynamiques** : injecter une URL ou une date dans Scrapy Ã  chaque exÃ©cution.
- **Bases de donnÃ©es** : Ã©crire les donnÃ©es dans PostgreSQL, Azure SQL, MongoDB...
- **Tests automatisÃ©s** : avec `pytest`, pour chaque spider.
- **Triggers externes** : lancer le DAG via une API ou webhook.

---

## Ressources utiles
- [ðŸ“˜ Scrapy Documentation](https://docs.scrapy.org/en/latest/)
- [ðŸ“˜ Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [ðŸ“˜ Azure Airflow on VM Guide](https://learn.microsoft.com/fr-fr/azure/container-instances/container-instances-airflow)




LAncer le dag:

```bash
docker-compose down --volumes --remove-orphans
docker-compose up --build
```

tester le code dans un shell:

```bash
docker exec -it airflow-webserver-1 bash 
````
```bash
python3 run_spider.py
````
